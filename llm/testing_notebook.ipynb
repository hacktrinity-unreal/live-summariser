{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from typing import Generator\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "os.system(\"sh ../.env\")\n",
    "YOUR_API_KEY = os.environ.get(\"YOUR_API_KEY\")\n",
    "print(YOUR_API_KEY)\n",
    "MODEL_NAME_HERE = \"llama-3.1-sonar-large-128k-online\"\n",
    "\n",
    "with open(\"comments.txt\",\"w\") as f:\n",
    "    pass\n",
    "with open(\"summary.txt\",\"w\") as f:\n",
    "    pass\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def convert_mp4_to_wav(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Convert an MP4 video file to a WAV audio file using FFmpeg.\n",
    "    \n",
    "    :param input_file: The input .mp4 file path.\n",
    "    :param output_file: The output .wav file path.\n",
    "    \"\"\"\n",
    "    # Use FFmpeg command to convert MP4 to WAV\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", input_file,      # Input file\n",
    "        \"-vn\",                  # No video\n",
    "        \"-acodec\", \"pcm_s16le\",  # Audio codec\n",
    "        \"-ar\", \"44100\",        # Sample rate\n",
    "        \"-ac\", \"2\",            # Number of audio channels\n",
    "        output_file            # Output file path\n",
    "    ]\n",
    "    \n",
    "    # Run the command\n",
    "    subprocess.run(command, check=True)\n",
    "    print(f\"Converted {input_file} to {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "input_mp4_file = \"/home/fedora/data/oj1.mp4\"\n",
    "output_wav_file = \"output.wav\"\n",
    "\n",
    "if not \"output.wav\" in os.listdir():convert_mp4_to_wav(input_mp4_file, output_wav_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 60000  # milliseconds\n",
    "\n",
    "\n",
    "\n",
    "def speech_to_text(audio_path: str) -> Generator[str, None, None]:\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    num_chunks = len(audio) // CHUNK_SIZE + (1 if len(audio) % CHUNK_SIZE > 0 else 0)\n",
    "    try:\n",
    "        os.mkdir(\n",
    "        \"temp\"\n",
    "        )\n",
    "    except:\n",
    "        os.system(\"temp/*\")\n",
    "\n",
    "    td= \"temp\"\n",
    "    for i in range(num_chunks):\n",
    "        start_time = i * CHUNK_SIZE\n",
    "        end_time = start_time + CHUNK_SIZE\n",
    "        chunk = audio[start_time:end_time]\n",
    "        chunk_filename = f\"{td}/chunk_{i}.wav\"\n",
    "        chunk.export(chunk_filename, format=\"wav\")\n",
    "\n",
    "        yield _recognise_text(chunk_filename)\n",
    "\n",
    "\n",
    "def _recognise_text(audio_path: str) -> str:\n",
    "    recogniser = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        data = recogniser.record(source)\n",
    "\n",
    "    text = recogniser.recognize_google(data)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = speech_to_text(\"output.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object speech_to_text at 0x7fb73485a680>\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def chunk_trancript(text):\n",
    "# Load GPT-4 tokenizer\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    max_tokens = 8000\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "    \n",
    "    text_chunks = [encoding.decode(chunk) for chunk in chunks]\n",
    "    return text_chunks\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_message(chunks):\n",
    "    prev_chunk = None\n",
    "    prev_summarization = None\n",
    "    messages = []\n",
    "    for i in range(len(chunks)):\n",
    "        current_chunk = chunks[i]\n",
    "        current_message = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a lawyer who summarises the transcript of legal cases. Be concise, accurate and understandable.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Summarize the current body of text, which is a chunk of a larger document '{current_chunk}'. Your summarization should contain all the important details in a readable form. Be concise. Here is the previus\"\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "        messages.append(current_message)\n",
    "    return messages\n",
    "def gen_single_message(chunk,text_response):\n",
    "    postfix = \"Here is the previous response '{text_response}'\" if  text_response  else \"\"\n",
    "    current_message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a lawyer who summarises the transcript of legal cases. Be concise, accurate and understandable. Do not include an introduction or a conclusion.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"Summarize the current body of text, which is a chunk of a larger document '{chunk}'. Your summarization should contain all the important details in a readable form. Be concise.\"+postfix\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    return current_message\n",
    "\n",
    "def gen_lawyer_message(summaries):\n",
    "    current_message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a lawyer who analyzes snippets court cases based on their transcripts/summaries. Provide insightful and interesting commentary on the proceedings of the cases. Keep your comments short and to the point, do not give an introduction or conclusion.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f'Analyze the following proceedings and give unique and interesting opinions on it. Clarify any points that may not be accessible to a general audience.\"{' '.join(summaries)}\"'\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    return current_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: line 1: temp/chunk_0.wav: Permission denied\n"
     ]
    },
    {
     "ename": "UnknownValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_response\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     text_chunks \u001b[38;5;241m=\u001b[39m chunk_trancript(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m     client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mYOUR_API_KEY, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.perplexity.ai\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     outputs,summaries \u001b[38;5;241m=\u001b[39m [],[]\n",
      "Cell \u001b[0;32mIn[64], line 23\u001b[0m, in \u001b[0;36mspeech_to_text\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m chunk_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m chunk\u001b[38;5;241m.\u001b[39mexport(chunk_filename, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_recognise_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 31\u001b[0m, in \u001b[0;36m_recognise_text\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mAudioFile(audio_path) \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[1;32m     29\u001b[0m     data \u001b[38;5;241m=\u001b[39m recogniser\u001b[38;5;241m.\u001b[39mrecord(source)\n\u001b[0;32m---> 31\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mrecogniser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_google\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/speech_recognition/recognizers/google.py:263\u001b[0m, in \u001b[0;36mrecognize_legacy\u001b[0;34m(recognizer, audio_data, key, language, pfilter, show_all, with_confidence, endpoint)\u001b[0m\n\u001b[1;32m    256\u001b[0m response_text \u001b[38;5;241m=\u001b[39m obtain_transcription(\n\u001b[1;32m    257\u001b[0m     request, timeout\u001b[38;5;241m=\u001b[39mrecognizer\u001b[38;5;241m.\u001b[39moperation_timeout\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    260\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m OutputParser(\n\u001b[1;32m    261\u001b[0m     show_all\u001b[38;5;241m=\u001b[39mshow_all, with_confidence\u001b[38;5;241m=\u001b[39mwith_confidence\n\u001b[1;32m    262\u001b[0m )\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/speech_recognition/recognizers/google.py:134\u001b[0m, in \u001b[0;36mOutputParser.parse\u001b[0;34m(self, response_text)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, response_text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 134\u001b[0m     actual_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_all:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m actual_result\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/speech_recognition/recognizers/google.py:183\u001b[0m, in \u001b[0;36mOutputParser.convert_to_result\u001b[0;34m(response_text)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m UnknownValueError()\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnknownValueError()\n",
      "\u001b[0;31mUnknownValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def summarize_chunk(chunk,text_response):\n",
    "    message = gen_single_message(chunk,text_response)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME_HERE,\n",
    "        messages=message,\n",
    "    )\n",
    "    text_response = response.choices[0].message.content.strip()\n",
    "    return text_response\n",
    "def comment_on_summaries(outputs):\n",
    "    message = gen_lawyer_message(outputs)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME_HERE,\n",
    "        messages=message,\n",
    "    )\n",
    "    text_response = response.choices[0].message.content.strip()\n",
    "    return text_response\n",
    "\n",
    "while True:\n",
    "    text_chunks = chunk_trancript(next(text))\n",
    "\n",
    "    client = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "    outputs,summaries = [],[]\n",
    "    text_response = None\n",
    "    # chat completion without streaming\n",
    "    for chunk in text_chunks:\n",
    "        text_response = summarize_chunk(chunk,text_response)\n",
    "        with open(\"summary.txt\",\"a\") as f:\n",
    "            f.write(text_response)\n",
    "        outputs.append(text_response)\n",
    "        lawyer_comment = comment_on_summaries(outputs)\n",
    "        with open(\"comments.txt\",\"a\") as f:\n",
    "            f.write(lawyer_comment)\n",
    "\n",
    "        summaries.append(lawyer_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
